<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>VideoA11y: Method and Datasets for Accessible Video Description</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css" rel="stylesheet">
    <link rel="stylesheet" href="css/style.css">
    <link rel="stylesheet" href="css/fontface.css">
</head>

<body>
    <div class="container text-center mt-5">
        <h1 class="title"><strong>VideoA11y: Method and Datasets for Accessible Video
                Description</strong></h1>
        <div class="button-group mt-3">
            <a href="downloads.html" class="btn btn-dark">
                <i class="fas fa-database"></i>Dataset
            </a>
            <a href="" class="btn button-secondary">
                <i class="fas fa-file-alt"></i> Paper
            </a>
            <a href="https://github.com/videoa11ygpt/VideoA11y" class="btn btn-dark">
                <i class="fab fa-github"></i>Code
            </a>
        </div>
    </div>
    <div class="article-text mt-5">
        <h2>Abstract</h2>
        <p>Video descriptions are crucial for blind and low vision (BLV) users to access visual content. However, current artificial intelligence models for generating descriptions often fall short due to limitations in the quality of human annotations within training datasets, resulting in descriptions that do not fully meet BLV users' needs. To address this gap, we introduce VideoA11y, an approach that leverages multimodal large language models (MLLMs) and video accessibility guidelines to generate descriptions specifically tailored for BLV individuals. Using this method, we have curated VideoA11y-40K, the largest and most comprehensive dataset of 40,000 videos described for BLV users. Rigorous experiments across 15 video categories, involving 347 sighted participants, 40 BLV participants, and seven professional audio describers, showed that descriptions generated by VideoA11y outperform original human annotations in clarity, accuracy, objectivity, descriptiveness, and user satisfaction. We evaluated models on VideoA11y-40K using both standard and custom metrics, demonstrating that MLLMs fine-tuned on this dataset produce high-quality accessible descriptions.
        </p>
    </div>
    <div class="article-text mt-5">
        <h2>Introduction</h2>
        <p>
            Video content is being created at an unprecedented rate, especially on platforms like YouTube and TikTok. However, this growth has widened the accessibility gap for blind and low-vision (BLV) users, who rely on audio descriptions (AD) to understand visual content. ADs narrate key visual elements like actions, characters, and scene transitions, but their availability is limited. While professionally produced media often include ADs due to regulations, user-generated content and independent productions lag significantly behind in providing accessibility.
        </p>
        <div class="container mt-3">
            <img src="assets/images/img1-example.png" alt="he human annotations and descriptions
            generated by VideoA11y for six consec- utive frames of two sample video clips. Red
            italics indicate the errors in human annotations, green underlining indicates the
            corrected facts, and blue bold indicates additional details.">
        </div>
        <p>
            To address these challenges, we developed VideoA11y, an innovative approach using advanced AI models to generate accessible video descriptions tailored for BLV individuals. Accompanied by the VideoA11y-40K dataset, this method leverages multimodal large language models (MLLMs) and curated accessibility guidelines to produce clear, detailed, and accurate descriptions. Evaluations involving sighted and BLV users demonstrate that descriptions created by VideoA11y outperform human annotations across key metrics like clarity, objectivity, and user satisfaction. This solution offers a scalable way to make video content more inclusive and enjoyable for all.
        </p>
        <p>
            Efforts to improve video accessibility have traditionally focused on tools like YouDescribe and Rescribe, which assist sighted individuals in creating ADs. While helpful, these systems require manual input, limiting scalability. Some approaches have introduced AI-generated video descriptions using image captioning or natural language processing, but these often fail to capture the broader context or handle temporal transitions effectively. The result is descriptions that lack accuracy, coherence, or alignment with accessibility needs.
        </p>
        <p>
            Recent advances in MLLMs and datasets, such as BLIP2 and Panda-70M, have brought improvements in automatic video understanding. However, they do not tailor their outputs to meet the specific requirements of BLV users. VideoA11y addresses this gap by incorporating professionally curated AD guidelines into the description generation process. By combining cutting-edge AI capabilities with human-centered design principles, VideoA11y provides a significant leap forward in creating meaningful and accessible video descriptions.
        </p>
    </div>
    <div class="article-text mt-5">
        <h2>Development of VideoA11y</h2>
        <h4>Overview</h4>
        <p>
            VideoA11y is designed to generate high-quality, accessible video descriptions using multimodal large language models (MLLMs) guided by accessibility-focused guidelines. At its core, the system relies on a set of 42 curated audio description (AD) guidelines that emphasize clarity, accuracy, and relevance, ensuring that descriptions meet the specific needs of blind and low-vision (BLV) users. These guidelines inform both the creation of the descriptions and the design of the evaluation metrics. The resulting descriptions are both comprehensive and well-structured, making them suitable for a wide range of video categories, including education, sports, entertainment, and instructional content.
        </p>
        <p>
            To assess and validate its effectiveness, VideoA11y was evaluated through extensive user studies involving sighted and BLV participants. These studies focused on key metrics such as descriptiveness, objectivity, accuracy, and clarity. Additionally, VideoA11y models were benchmarked against state-of-the-art AI models using both standard metrics like BLEU and custom metrics tailored to accessibility. Results consistently demonstrated that VideoA11y-produced descriptions outperform traditional human annotations and baseline AI models, establishing it as a robust and scalable framework for improving video accessibility.
        </p>
        <div class="container mt-3">
            <img src="assets/images/img2-example.jpg" alt="Overview of the VideoA11y pipeline.
            First, keyframes are extracted from the input video. Then, the keyframes, the prompt, AD
            guidelines, and optional human annotations are provided to GPT-4V, which subsequently
            generates accessible video descriptions.">
        </div>
        <h4>Methodology</h4>
        <p>
            VideoA11y uses multimodal large language models (MLLMs) and curated audio description (AD) guidelines to create video descriptions tailored for BLV users. The method begins with extracting keyframes—significant moments in a video—using a luminance-based algorithm that identifies visual changes. These keyframes are processed by MLLMs like GPT-4 Vision, which generate or refine descriptions based on carefully crafted prompts adhering to 42 professional AD guidelines.
        </p>
        <div class="citation-container">
            <pre><code>
Imagine your role is to generate descriptions for videos to make them accessible to blind and low vision individuals. You will watch a sequence of keyframes from a video and read the current description of this video. Your task is to revise the current description. You must follow all the given instructions. Output your result in a dictionary format: {“Video_Category”: A string representing the category of video you believe it to be, “Revised_Desc”: A string of revised description.}

Current Description: {desc_current}

Instructions:
Instruction #1: Avoid over-describing — Do not include non-essential visual details.
Instruction #2: Description should not be opinionated unless content demands it.
Instruction #3: ...
            </code></pre>
        </div>
        <p>
            The guidelines were distilled from 154 industry standards, prioritizing clarity, relevance, and objectivity. Prompts guide the MLLM to produce descriptions aligned with these standards, improving detail and accuracy while minimizing extraneous or biased content. Evaluations showed GPT-4 Vision as the optimal model for generating descriptions, laying the foundation for creating the VideoA11y-40K dataset.
        </p>
        <h4>Study 1</h4>
        <p>
            Study 1 evaluated the performance of VideoA11y using both open-source (Video-LLaVA) and proprietary (GPT-4 Vision) multimodal large language models (MLLMs). The goal was to determine which model best generates video descriptions adhering to curated AD guidelines. A dataset of 150 videos across 15 categories (e.g., sports, education) was used, and descriptions were generated under four conditions:
          </p>
          <ul>
            <li>
              <strong>VideoA11y (LLaVA) without Human Annotations (HA):</strong> Generated descriptions using Video-LLaVA and AD-guideline-based prompts.
            </li>
            <li>
              <strong>VideoA11y (LLaVA) with HA:</strong> Enhanced Video-LLaVA descriptions by including human annotations as a reference.
            </li>
            <li>
              <strong>VideoA11y (GPT-4 Vision) without HA:</strong> Generated descriptions using GPT-4 Vision and guideline-based prompts.
            </li>
            <li>
              <strong>VideoA11y (GPT-4 Vision) with HA:</strong> Enhanced GPT-4 Vision descriptions with human annotations.
            </li>
          </ul>
          <p>
            150 sighted participants rated the descriptions on clarity, descriptiveness, accuracy, and objectivity using a 5-point scale. Results showed that descriptions generated by <strong>VideoA11y (GPT-4 Vision)</strong> significantly outperformed all other conditions across all metrics. Adding human annotations slightly improved quality but was not statistically significant. Based on these findings, GPT-4 Vision was selected as the core model for creating the VideoA11y-40K dataset.
          </p>
          <div class="container mt-3">
            <img src="assets/images/study-1.png" alt="Study 1 results comparing VideoA11y (GPT-4 Vision) with other conditions. GPT-4 Vision outperformed all other conditions in clarity, descriptiveness, accuracy, and objectivity.">
          </div>
    </div>
    <div class="article-text mt-5">
        <h2>VideoA11y-40K Dataset</h2>
        <p>
            The VideoA11y-40K dataset is a comprehensive collection of 40,000 videos across 15 categories, ranging from sports to instructional content. Descriptions in the dataset were generated using GPT-4 Vision and follow the curated AD guidelines to ensure relevance and precision. Each description averages 52 words, significantly longer and more detailed than those in traditional datasets, which average 20 words.
        </p>
        <img src="assets/images/dataset.png" alt="Distribution of videos across categories in the VideoA11y-40K dataset and proportions of video descriptions by word count.">
        <p>
            The dataset’s categories were validated by human reviewers, achieving 96% accuracy in classification. Its detailed and scalable nature makes VideoA11y-40K a valuable resource for training AI models to create accessible video content. By incorporating AD best practices, the dataset supports the development of systems capable of delivering meaningful video descriptions at scale.
        </p>
    </div>
    <div class="article-text mt-5">
        <h2>Evaluation of VideoA11y</h2>
        <p>
            VideoA11y underwent rigorous testing with sighted and BLV participants. Sighted users rated descriptions across clarity, descriptiveness, accuracy, and objectivity. Results showed that VideoA11y significantly outperformed novice human annotations and even matched or surpassed trained human describers, particularly in clarity. Among BLV users, VideoA11y’s impact was pronounced. In a study involving 40 participants across five video categories, over 90% preferred descriptions generated by VideoA11y. Participants praised its clarity, detailed synchronization with video events, and unbiased presentation. These results demonstrate that VideoA11y not only improves comprehension but also enhances the viewing experience for BLV individuals.
        </p>
        <img src="assets/images/study-2.png" alt="Results of Study 2 with 150 sighted MTurk users. VideoA11y outperforms other methods in all metrics (p < 0.001), followed by VideoA11y w/o HA. HA: Human Annotation.">
        <h4>Study 2</h4>
        <p>
            Study 2 was designed to assess the quality of descriptions generated by VideoA11y compared to novice human annotations from existing datasets. The study also included a comparison with GPT-4 Vision-generated descriptions created using non-compliant prompts (i.e., prompts not following AD guidelines). The goal was to evaluate how well VideoA11y performs in terms of clarity, descriptiveness, accuracy, and objectivity.
          </p>
          <p>
            The evaluation involved 150 new participants recruited through Amazon Mechanical Turk (MTurk). Each participant reviewed two videos and rated five types of descriptions for each video:
          </p>
          <ul>
            <li>
              <strong>Human Annotation:</strong> Descriptions created by novice human annotators in the original datasets.
            </li>
            <li>
              <strong>GPT-4V:</strong> Descriptions generated by GPT-4 Vision using non-compliant prompts.
            </li>
            <li>
              <strong>GPT-4V with Human Annotations:</strong> GPT-4 Vision descriptions enhanced with novice human annotations.
            </li>
            <li>
              <strong>VideoA11y without Human Annotations:</strong> Descriptions generated using VideoA11y’s compliant prompts without leveraging human annotations.
            </li>
            <li>
              <strong>VideoA11y with Human Annotations:</strong> VideoA11y descriptions created using compliant prompts with human annotations as a reference.
            </li>
          </ul>
          <p>
            Participants rated each description on a 5-point scale across the four evaluation metrics. Results showed that <strong>VideoA11y with Human Annotations</strong> significantly outperformed all other methods on all metrics, with ratings above 4.0 on clarity, descriptiveness, accuracy, and objectivity. <strong>VideoA11y without Human Annotations</strong> also performed exceptionally well, surpassing novice human annotations and GPT-4V-generated descriptions in all metrics.
          </p>
          <p>
            These findings highlight the importance of adhering to AD guidelines in generating accessible video descriptions. By leveraging these guidelines and human annotations, VideoA11y provides a robust solution for creating meaningful and accurate descriptions for BLV users.
          </p>
          <h4>Study 3</h4>
          <p>
            Study 3 evaluated the quality of VideoA11y descriptions compared to those created by trained human annotators. This study aimed to assess whether descriptions generated by VideoA11y could meet or exceed the standards of professional audio describers who followed curated AD guidelines.
          </p>
          <p>
            A total of 47 videos were selected from YouTube, spanning a variety of categories such as sports, instructional content, and entertainment. These videos had an average duration of 4.92 minutes. A team of four accessibility researchers carefully crafted descriptions for these videos, adhering strictly to the 42 professional AD guidelines compiled for this project. These human-generated descriptions were then compared to descriptions created by VideoA11y using GPT-4 Vision.
          </p>
          <img src="assets/images/study-3.png" alt="Results of Study 3 with 47 sighted MTurk users. VideoA11y outperformed trained humans on the ‘clear’ metric (p = 0.004) and performed similarly to trained humans on other metrics (p > 0.05).">
          <p>
            To evaluate the descriptions, 47 sighted participants from Amazon Mechanical Turk (MTurk) were recruited. Each participant reviewed descriptions generated by both methods for the same video and rated them on four key metrics: descriptiveness, objectivity, accuracy, and clarity. The participants used a 5-point scale to provide their ratings.
          </p>
          <p>
            Results showed that VideoA11y descriptions matched or exceeded the quality of trained human annotations in most metrics. Notably, VideoA11y outperformed trained human annotators on the clarity metric, with a statistically significant improvement (<em>p = 0.004</em>). Participants highlighted that VideoA11y descriptions were often clearer, better synchronized with the visual content, and provided unbiased and detailed accounts of the videos.
          </p>
          <p>
            These findings demonstrate that VideoA11y can generate video descriptions that rival professional standards, making it a valuable tool for scaling video accessibility without compromising quality.
          </p>
          <h4>Study 4</h4>
          <p>
            Study 4 focused on evaluating the effectiveness of VideoA11y and its impact on the video-watching 
            experience of blind and low vision (BLV) individuals. This study aimed to compare descriptions 
            generated by VideoA11y to novice human annotations and assess user preferences and satisfaction.
          </p>
          <p>
            A total of 40 BLV participants were recruited, including six completely blind and 34 legally blind 
            individuals with varying degrees of visual impairment. Participants were divided into two groups of 
            20, with each group evaluating five videos across five categories: entertainment, how-to, sports, 
            pets and animals, and people and vlogs. For each video, participants experienced both human-generated 
            descriptions and VideoA11y-generated descriptions, presented in a counterbalanced order.
          </p>
          <img src="assets/images/study-4-1.png" alt="VideoA11y outperforms novice human annotations in all five video
          categories and outperforms novice human annotations in all four metrics.">
          <p>
            Participants rated the descriptions on a 10-point Likert scale across four metrics: descriptiveness, 
            objectivity, accuracy, and clarity. They also indicated their preferred description for each video 
            and provided feedback on why they made their choice. Descriptions were embedded into the videos as 
            audio, following best practices for accessibility.
          </p>
          <p>
            The results were highly positive for VideoA11y. Participants preferred VideoA11y-generated descriptions 
            in over 90% of cases, with an overall selection rate of 180 out of 200 comparisons. VideoA11y achieved 
            significantly higher ratings than novice human annotations on all metrics. Comments from BLV participants 
            highlighted the clarity, detail, and alignment of VideoA11y descriptions with the visual content, 
            enhancing their understanding and enjoyment of the videos.
          </p>
          <img src="assets/images/study-4-2.png" alt="Overall pairwise comparisons from BLV user evaluations between VideoA11y and novice human descriptions in Study 4.">
          <p>
            This study underscores the ability of VideoA11y to meet the unique needs of BLV users, providing accurate, 
            clear, and descriptive audio that enhances their accessibility to video content.
          </p>
    </div>
    <div class="article-text mt-5">
        <h2>Technical Benchmarks</h2>
        <h4>Overview of Benchmarking</h4>
        <p>
            VideoA11y was evaluated using a benchmark designed to test the performance of state-of-the-art (SOTA) open-source models in generating accessible video descriptions. These experiments compared baseline models to fine-tuned versions trained on the VideoA11y-40K dataset. The goal was to determine whether training on VideoA11y-40K improved model performance on both standard and custom metrics.
        </p>
        <h4>Baseline Models</h4>
        <p>Four SOTA open-source models were selected for comparison:</p>
        <ul>
            <li><strong>Video-LLaVA-7B:</strong> A smaller, lightweight model.</li>
            <li><strong>VILA1.5-40B:</strong> Focused on cross-modality analysis.</li>
            <li><strong>LLaVA-NeXT-Video-32B:</strong> Specialized in understanding temporal changes.</li>
            <li><strong>LLaVA-OneVision-72B:</strong> A large-scale model for video comprehension.</li>
        </ul>
        <p>
            Each model was evaluated under its original settings, including the number of frames processed and inference parameters. These served as baselines to assess the impact of fine-tuning on VideoA11y-40K.
        </p>
        <h4>Fine-Tuned Models</h4>
        <p>
            Two open-source models, <strong>Video-LLaVA-7B</strong> and <strong>LLaVA-NeXT-Video-32B</strong>, were fine-tuned on VideoA11y-40K. This involved LoRA fine-tuning, which adjusts specific model layers while preserving pre-trained knowledge. The fine-tuning process included:
        </p>
        <ul>
            <li>10 epochs</li>
            <li>Learning rate of 2e-5</li>
            <li>Batch size of 4 per device</li>
            <li>Support for up to 32,768 token lengths</li>
        </ul>
        <p>The fine-tuned models were named <strong>VideoA11y-7B</strong> and <strong>VideoA11y-32B</strong>.</p>
        <h4>Evaluation Metrics</h4>
        <p>Evaluations were conducted using both standard NLP metrics and custom accessibility-focused metrics:</p>
        <ul>
            <li>
            <strong>Standard Metrics:</strong> BLEU, METEOR, ROUGE, CIDEr, and SPICE, which assess text coherence, relevance, and alignment with human judgment.
            </li>
            <li>
            <strong>Custom Metrics:</strong> Descriptiveness, objectivity, accuracy, and clarity, rated on a scale of 1–5. These metrics reflect the specific needs of BLV users.
            </li>
        </ul>
        <h4>Benchmarking Results</h4>
        <img src="assets/images/scores-1.png" alt="Comparison of standard NLP metrics for di!erent models on a held-out test set. Bold number indicate the best performance, and underlined number indicate the second best performance.">
        <p>
            Fine-tuned models consistently outperformed baseline models across all metrics. Key findings include:
        </p>
        <ul>
            <li>
            <strong>Standard Metrics:</strong> VideoA11y-32B achieved the highest scores in BLEU, METEOR, CIDEr, and SPICE, followed by VideoA11y-7B. These models demonstrated improved semantic richness and alignment with ground truth.
            </li>
            <li>
            <strong>Custom Metrics:</strong> Evaluations using GPT-4 as an evaluator showed that VideoA11y-32B achieved the highest ratings on descriptiveness, objectivity, accuracy, and clarity. VideoA11y-7B also performed well, surpassing baseline models.
            </li>
        </ul>
        <img src="assets/images/scores-2.png" alt="Comparison of custom metrics for di!erent models on a held-out test set. Bold number indicate the best performance, and underlined number indicate the second best performance.">
        <h4>Implications</h4>
        <p>
            Fine-tuning on VideoA11y-40K significantly enhances the capability of open-source models to generate high-quality, accessible video descriptions. While the fine-tuned models are not yet at the level of proprietary systems like GPT-4 Vision, they offer a scalable and cost-effective solution for video accessibility. This makes them ideal for platforms with large volumes of video content.
        </p>
    </div>
    <div class="article-text mt-5">
        <h2>Conclusion and Future Discussions</h2>
        <p>
            VideoA11y addresses a critical gap in video accessibility by leveraging advanced AI and professionally curated guidelines. Its scalable method and comprehensive dataset enable the creation of high-quality video descriptions tailored to BLV users. User evaluations confirmed its superior performance over human-generated descriptions, making it a transformative tool for inclusive content.
        </p>
        <p>
            Future work will explore personalization, such as adapting descriptions to individual preferences and integrating inline descriptions that seamlessly fit video pacing. These advancements will ensure that VideoA11y continues to lead in video accessibility, creating a richer experience for BLV users and setting a new standard in inclusive technology.
        </p>
    </div>




    <div class="article-text mt-5">
        <h2>Sample Videos</h2>
        <p>Below are a few videos from the VideoA11y dataset with revised descriptions using VideoA11y.</p>
    </div>
    <div class="video-container">
        <button id="prev" class="btn">&#10094;</button>
        <video id="videoPlayer1" controls>
            <source src="assets/videos/vid1-example.mp4" type="video/mp4">
            Your browser does not support the video tag.
        </video>
        <div id="caption1" class="caption-box">
            <h5 class="cap-title">Video #1</h5>
            <p><strong>Video ID:</strong> FveaOFL7eOs</p>
            <p><strong>VideoA11y:</strong> A person in a white protective beekeeping suit
                inspects a large rectangular honeycomb frame. The frame is mostly covered with a pattern
                of hexagonal cells, some filled with honey, while others are empty. The beekeeper holds
                the frame with both hands, examining it closely. Bees crawl across the surface, their
                buzzing audible. Text on the screen questions the absence of eggs or larvae in the brood
                chamber.</p>
        </div>
        <video id="videoPlayer2" controls style="display:none;">
            <source src="assets/videos/vid2-example.mp4" type="video/mp4">
            Your browser does not support the video tag.
        </video>
        <div id="caption2" class="caption-box" style="display:none;">
            <h5 class="cap-title">Video #2</h5>
            <p><strong>Video ID:</strong> jjOqzXCwDec</p>
            <p><strong>VideoA11y:</strong> A tabby cat peeks through a small, square pet door
                built into a glass patio door, then steps out onto a brick patio. The scene shifts to
                the cat confidently striding towards the camera. Subsequent frames show the brand name
                'SUREFLAP' in bold white letters on a blue background, followed by the slogan 'SureFlap
                pets are happy pets' in white script.</p>
        </div>
        <video id="videoPlayer3" controls style="display:none;"> 
            <source src="assets/videos/vid3-example.mp4" type="video/mp4">
            Your browser does not support the video tag.
        </video>
        <div id="caption3" class="caption-box" style="display:none;">
            <h5 class="cap-title">Video #3</h5>
            <p><strong>Video ID:</strong> ucaCmhNo78k</p>
            <p><strong>VideoA11y:</strong> The video demonstrates the preparation of a cheesy
                hash brown casserole. The cook starts by greasing a clear glass baking dish. Cream of
                chicken soup is poured into the dish, followed by the addition of melted margarine. The
                cook then sprinkles shredded Colby cheese into the mixture. After each ingredient is
                added, the cook mixes them thoroughly. Diced onions are scattered over the mixture,
                seasoned with salt and black pepper, and stirred in. Finally, shredded hash browns are
                combined with the rest of the ingredients. The cook ensures an even distribution of all
                components before transferring the dish to the oven to bake until golden brown. The
                finished casserole is served on a plate, showcasing a crispy golden top with a creamy,
                cheesy interior.</p>
        </div>
        <button id="next" class="btn">&#10095;</button>
    </div>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
        let currentVideoIndex = 1;
        const totalVideos = 4;

        function showVideo(index) {
            for (let i = 1; i <= totalVideos; i++) {
            const video = document.getElementById(`videoPlayer${i}`);
            const caption = document.getElementById(`caption${i}`);
            if (video && caption) {
                video.style.display = "none";
                caption.style.display = "none";
            }
            }

            document.getElementById(`videoPlayer${index}`).style.display = "block";
            document.getElementById(`caption${index}`).style.display = "block";

            updateButtonVisibility(index);
        }

        function updateButtonVisibility(index) {
            const prevButton = document.getElementById("prev");
            const nextButton = document.getElementById("next");

            if (index === 1) {
            prevButton.style.display = "none";
            } else {
            prevButton.style.display = "inline-block";
            }
            if (index === totalVideos) {
            nextButton.style.display = "none";
            } else {
            nextButton.style.display = "inline-block";
            }
        }

        document.getElementById("next").addEventListener("click", function () {
            if (currentVideoIndex < totalVideos) {
            currentVideoIndex++;
            showVideo(currentVideoIndex);
            }
        });

        document.getElementById("prev").addEventListener("click", function () {
            if (currentVideoIndex > 1) {
            currentVideoIndex--;
            showVideo(currentVideoIndex);
            }
        });

        showVideo(currentVideoIndex);
        });
    </script>
    <!-- Uncomment for Citation

    <div class="article-text mt-5">
        <h2>Citation</h2>
        <p>Below is the citation for our paper. Feel free to copy it by clicking on it!</p>
    </div>
    <div class="citation-container" id="bibtexContainer" title="Click to copy">
        <span class="tooltip">Click to copy citation</span>
        <pre id="bibtex">
    @inproceedings{Li2024VideoA11y, title={VideoA11y: Method and Datasets for Accessible Video
    Description}, author={Li, Chaoyu and Padmanabhuni, Sid and Cheema, Maryam and Xu, Chenliang and
    Seifi, Hasti and Fazli, Pooyan}, year={2024} }
        </pre>
    </div>
    <script>
        document.getElementById('bibtexContainer').addEventListener('click', function() {
            const bibtexText = document.getElementById('bibtex').innerText; // Get the text content
            navigator.clipboard.writeText(bibtexText).then(() => {
                alert('Citation copied to clipboard!'); // Success feedback
            }).catch(err => {
                console.error('Failed to copy: ', err); // Error handling
            });
        });
    </script>         -->
    <div class="article-text mt-5">
        <h2>Acknowledgments</h2>
        <p>
            This research was supported by the National Eye Institute (NEI) of the National Institutes of Health
            (NIH) under award number R01EY034562. The content is solely the responsibility of the authors
            and does not necessarily represent the official views of the NIH.
        </p>
    </div>
    <footer class="footer">
        <div class="container text-center">
            © 2024 People and Robots Lab
        </div>
    </footer>
</body>

</html>
