<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>VideoA11y: Method and Datasets for Accessible Video Description</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css" rel="stylesheet">
    <link rel="stylesheet" href="css/style.css">
    <link rel="stylesheet" href="css/fontface.css">
</head>

<body>
    <div class="container text-center mt-5">
        <h1 class="title"><strong>VideoA11y: Method and Datasets for Accessible Video
                Description</strong></h1>
        <div class="button-group mt-3">
            <a href="downloads.html" class="btn btn-dark">
                <i class="fas fa-database"></i>Dataset
            </a>
            <a href="" class="btn button-secondary">
                <i class="fas fa-file-alt"></i> Paper
            </a>
            <a href="https://github.com/videoa11ygpt/VideoA11y" class="btn btn-dark">
                <i class="fab fa-github"></i>Code
            </a>
        </div>
    </div>
    <div class="article-text mt-5">
        <h2>Abstract</h2>
        <p>Video descriptions are crucial for blind and low vision (BLV) users to access visual content. However, current artificial intelligence models for generating descriptions often fall short due to limitations in the quality of human annotations within training datasets, resulting in descriptions that do not fully meet BLV users' needs. To address this gap, we introduce VideoA11y, an approach that leverages multimodal large language models (MLLMs) and video accessibility guidelines to generate descriptions specifically tailored for BLV individuals. Using this method, we have curated VideoA11y-40K, the largest and most comprehensive dataset of 40,000 videos described for BLV users. Rigorous experiments across 15 video categories, involving 347 sighted participants, 40 BLV participants, and seven professional audio describers, showed that descriptions generated by VideoA11y outperform original human annotations in clarity, accuracy, objectivity, descriptiveness, and user satisfaction. We evaluated models on VideoA11y-40K using both standard and custom metrics, demonstrating that MLLMs fine-tuned on this dataset produce high-quality accessible descriptions.
        </p>
    </div>
    <div class="article-text mt-5">
        <h2>Introduction</h2>
        <p>
            New video content is created at an astounding rate, further widening the digital accessibility (a11y) gap experienced by blind and low vision (BLV) people. Video description, also known as audio description (AD), can make videos accessible to BLV users by narrating the visual content of a scene, such as actions, characters, scene changes, and interactions. For professionally created media, such as films and television shows, producing ADs requires significant collaborative efforts from a team of experts, including producers, audio description writers, voice actors, and audio engineers. Audio descriptions for movies have become more prevalent over the years, particularly in countries with strong accessibility regulations. However, smaller studios and independent films may not always provide AD. In contrast, for user-generated content, which has surged in popularity on platforms such as YouTube and TikTok, the implementation of ADs lags considerably behind. Given the rapid increase in online videos, human description alone is insufficient, making artificial intelligence (AI)-generated audio descriptions a viable alternative.
        </p>
        <div class="container mt-3">
            <img src="assets/images/img1-example.png" alt="he human annotations and descriptions
            generated by VideoA11y for six consec- utive frames of two sample video clips. Red
            italics indicate the errors in human annotations, green underlining indicates the
            corrected facts, and blue bold indicates additional details.">
        </div>
        <p>
            In recent years, advances in computer vision and natural language processing (NLP) have enabled the development of new techniques for automatically generating video descriptions using multimodal large language models (MLLMs). These models are typically trained on general video description datasets, which include videos paired with descriptions or annotations (in this paper, we use the terms `description' and `annotation' interchangeably) created by either humans or AI. However, existing datasets are insufficient for generating descriptions that effectively support BLV individuals in understanding video content. A key limitation is that the annotations in these datasets often contain errors and fail to adhere to AD guidelines for accessibility. Human-generated descriptions also tend to be brief and can occasionally contain grammatical, spelling, or semantic errors, which may limit the comprehension of the video for BLV audiences.    
        </p>
        <p>
            To address this gap, we introduce the VideoA11y method and the VideoA11y-40K dataset. VideoA11y is a novel approach designed to generate high-quality descriptions from scratch or enhance existing annotations for a wide range of video categories. It aims to produce detailed and accurate descriptions, thereby improving content accessibility for BLV users. To this end, we have compiled and summarized 42 guidelines from professional AD sources that capture the needs of BLV individuals. We then leveraged MLLMs to generate accessible descriptions using a prompt that adheres to these guidelines (i.e., compliant prompt). We used VideoA11y to curate VideoA11y-40K, the largest and most comprehensive video description dataset for training accessible models. The dataset includes 40,000 videos across 15 categories, all specifically described for BLV individuals.
        <p>
            We conducted five user studies with both sighted and BLV individuals. The results of these studies demonstrate that VideoA11y produces video descriptions of superior quality in all metrics. Finally, we developed a new video accessibility benchmark that uses VideoA11y-40K to evaluate open-source MLLMs on standard NLP metrics and the four custom metrics of descriptiveness, objectivity, accuracy, and clarity. We fine-tuned two open-source MLLMs on VideoA11y-40K and demonstrated that descriptions generated by these fine-tuned models outperform those produced by baseline models. Our work is pioneering in the HCI and AI community, focusing on creating a video description dataset specifically for BLV users and validating it with both sighted and BLV individuals. The novelty of this work lies in bridging established human practices of audio description with advancements in video description models, and in creating a method, dataset, and benchmark dedicated to video accessibility.
        </p>
    </div>
    <div class="article-text mt-5">
        <h2>Development of VideoA11y</h2>
        <h4>Overview</h4>
        <p>
            VideoA11y is designed to generate high-quality, accessible video descriptions using multimodal large language models (MLLMs) guided by accessibility-focused guidelines. At its core, the system relies on a set of 42 curated audio description (AD) guidelines that emphasize clarity, accuracy, and relevance, ensuring that descriptions meet the specific needs of blind and low-vision (BLV) users. These guidelines inform both the creation of the descriptions and the design of the evaluation metrics. The resulting descriptions are both comprehensive and well-structured, making them suitable for a wide range of video categories.
        </p>
        <p>
            To assess and validate its effectiveness, VideoA11y was evaluated through extensive user studies involving sighted and BLV participants. These studies focused on key metrics such as descriptiveness, objectivity, accuracy, and clarity. Additionally, VideoA11y models were benchmarked against state-of-the-art AI models using both standard metrics like BLEU and custom metrics tailored to accessibility. Results consistently demonstrated that VideoA11y-produced descriptions outperform traditional human annotations and baseline AI models, establishing it as a robust and scalable framework for improving video accessibility.
        </p>
        <div class="container mt-3">
            <img src="assets/images/img2-example.jpg" alt="Overview of the VideoA11y pipeline.
            First, keyframes are extracted from the input video. Then, the keyframes, the prompt, AD
            guidelines, and optional human annotations are provided to GPT-4V, which subsequently
            generates accessible video descriptions.">
        </div>
        <h4>Methodology</h4>
        <p>
            VideoA11y uses multimodal large language models (MLLMs) and curated audio description (AD) guidelines to create video descriptions tailored for BLV users. The method begins with extracting keyframes—significant moments in a video—using a luminance-based algorithm that identifies visual changes. These keyframes are processed by MLLMs like GPT-4 Vision, which generate or refine descriptions based on carefully crafted prompts adhering to 42 professional AD guidelines.
        </p>
        <div class="citation-container">
            <pre><code>
Imagine your role is to generate descriptions for videos to make them accessible to blind and low vision individuals. You will watch a sequence of keyframes from a video and read the current description of this video. Your task is to revise the current description. You must follow all the given instructions. Output your result in a dictionary format: {“Video_Category”: A string representing the category of video you believe it to be, “Revised_Desc”: A string of revised description.}

Current Description: {desc_current}

Instructions:
Instruction #1: Avoid over-describing — Do not include non-essential visual details.
Instruction #2: Description should not be opinionated unless content demands it.
Instruction #3: ...
            </code></pre>
        </div>
        <p>
            The guidelines were distilled from 154 industry standards, prioritizing clarity, relevance, and objectivity. Prompts guide the MLLM to produce descriptions aligned with these standards, improving detail and accuracy while minimizing extraneous or biased content. Evaluations showed GPT-4 Vision as the optimal model for generating descriptions, laying the foundation for creating the VideoA11y-40K dataset.
        </p>
        <h4>Study 1</h4>
        <p>
            Study 1 evaluated the performance of VideoA11y using both open-source (Video-LLaVA) and proprietary (GPT-4 Vision) multimodal large language models (MLLMs). The goal was to determine which model best generates video descriptions adhering to curated AD guidelines. A dataset of 150 videos across 15 categories (e.g., sports, education) was used, and descriptions were generated under four conditions:
          </p>
          <ul>
            <li>
              <strong>VideoA11y (LLaVA) w/o HA</strong> uses the compliant prompt and Video-LLaVA to generate video descriptions.
            </li>
            <li>
              <strong>VideoA11y (LLaVA)</strong> uses the compliant prompt with human annotations and Video-LLaVA to generate video descriptions.
            </li>
            <li>
              <strong>VideoA11y (GPT) w/o HA</strong> uses the compliant prompt and GPT-4V to generate video descriptions.
            </li>
            <li>
              <strong>VideoA11y (GPT)</strong> uses the compliant prompt with human annotations and GPT-4V to generate video descriptions.
            </li>
          </ul>
          <p>
            150 sighted participants rated the descriptions on clarity, descriptiveness, accuracy, and objectivity using a 5-point scale. The test reveals a significant effect of the description method. Pairwise comparisons indicate that VideoA11y (GPT) w/o HA and VideoA11y (GPT) significantly outperform both VideoA11y (LLaVA) w/o HA and VideoA11y (LLaVA) in all four metrics. The results also suggest that using human annotations as references can enhance the quality of descriptions, although not significantly. Based on these results, we selected GPT-4V as the MLLM and also incorporated the existing human annotations in creating the VideoA11y-40K dataset.
          </p>
          <div class="container mt-3">
            <img src="assets/images/study-1.png" alt="Study 1 results comparing VideoA11y (GPT-4 Vision) with other conditions. GPT-4 Vision outperformed all other conditions in clarity, descriptiveness, accuracy, and objectivity.">
          </div>
    </div>
    <div class="article-text mt-5">
        <h2>VideoA11y-40K Dataset</h2>
        <p>
            We employed VideoA11y (GPT) to generate high-quality video descriptions for the VideoA11y-40K dataset, which includes descriptions for 40,000 videos (32,000 training set, 4,000 validation set, and 4,000 test set) across 15 categories specifically tailored to BLV users. Each description averages 52 words, significantly longer and more detailed than those in traditional datasets, which average 20 words.
        </p>
        <img src="assets/images/dataset.png" alt="Distribution of videos across categories in the VideoA11y-40K dataset and proportions of video descriptions by word count.">
        <p>
            The dataset’s categories were validated by human reviewers, achieving 96% accuracy in classification. Its detailed and scalable nature makes VideoA11y-40K a valuable resource for training AI models to create accessible video content. By incorporating AD best practices, the dataset supports the development of systems capable of delivering meaningful video descriptions at scale.
        </p>
    </div>
    <div class="article-text mt-5">
        <h2>Evaluating VideoA11y Method and VideoA11y-40K Dataset with Sighted and BLV Users</h2>
        <p>
            VideoA11y underwent rigorous testing with sighted and BLV participants. 347 novice sighted users and seven professional audio describes rated descriptions across clarity, descriptiveness, accuracy, and objectivity. Results showed that VideoA11y significantly outperformed novice human annotations and even matched or surpassed trained human describers. Among BLV users, VideoA11y’s impact was pronounced. In a study involving 40 participants across five video categories, over 90% preferred descriptions generated by VideoA11y. Participants praised its clarity, detailed synchronization with video events, and unbiased presentation. These results demonstrate that VideoA11y not only improves comprehension but also enhances the viewing experience for BLV individuals.
        </p>
        <img src="assets/images/study-2.png" alt="Results of Study 2 with 150 sighted MTurk users. VideoA11y outperforms other methods in all metrics (p < 0.001), followed by VideoA11y w/o HA. HA: Human Annotation.">
        <h4>Study 2</h4>
        <p>
            Study 2 was designed to assess the quality of descriptions generated by VideoA11y compared to novice human annotations from existing datasets. The study also included a comparison with GPT-4 Vision-generated descriptions created using non-compliant prompts (i.e., prompts not following AD guidelines). The goal was to evaluate how well VideoA11y performs in terms of clarity, descriptiveness, accuracy, and objectivity.
          </p>
          <p>
            The evaluation involved 150 new participants recruited through Amazon Mechanical Turk (MTurk). Each participant reviewed two videos and rated five types of descriptions for each video:
          </p>
          <ul>
            <li>
              <strong>Human Annotation</strong> uses novice human annotations from the original datasets.
            </li>
            <li>
              <strong>GPT-4V</strong> uses the non-compliant prompt to generate video descriptions.
            </li>
            <li>
              <strong>GPT-4V w/ HA</strong> uses the non-compliant prompt with human annotations to generate video descriptions.
            </li>
            <li>
              <strong>VideoA11y w/o HA</strong> uses the compliant prompt to generate video descriptions.
            </li>
            <li>
              <strong>VideoA11y</strong> uses the compliant prompt with human annotations to generate video descriptions.
            </li>
          </ul>
          <p>
            Participants rated each description on a 5-point scale across the four evaluation metrics. Results showed that <strong>VideoA11y</strong> significantly outperformed all other methods on all metrics, with ratings above 4.0 on clarity, descriptiveness, accuracy, and objectivity. <strong>VideoA11y without Human Annotations</strong> also performed exceptionally well, surpassing novice human annotations and GPT-4V-generated descriptions in all metrics. These results indicate the effectiveness of AD guidelines in improving description quality beyond novice human annotations.
          </p>
          <h4>Study 3</h4>
          <p>
            Study 3 evaluated the quality of VideoA11y descriptions compared to those created by trained human annotators. This study aimed to assess whether descriptions generated by VideoA11y could meet or exceed the standards of professional audio describers who followed curated AD guidelines.
          </p>
          <p>
            A total of 47 videos were selected from YouTube, spanning a variety of categories such as sports, instructional content, and entertainment. These videos had an average duration of 4.92 minutes. A team of four accessibility researchers carefully crafted descriptions for these videos, adhering strictly to the 42 professional AD guidelines compiled for this project. These human-generated descriptions were then compared to descriptions created by VideoA11y using GPT-4 Vision.
          </p>
          <img src="assets/images/study-3.png" alt="Results of Study 3 with 47 sighted MTurk users. VideoA11y outperformed trained humans on the ‘clear’ metric (p = 0.004) and performed similarly to trained humans on other metrics (p > 0.05).">
          <p>
            To evaluate the descriptions, 47 sighted participants from Amazon Mechanical Turk (MTurk) were recruited. Each participant reviewed descriptions generated by both methods for the same video and rated them on four key metrics: descriptiveness, objectivity, accuracy, and clarity. The participants used a 5-point scale to provide their ratings.
          </p>
          <p>
            Results showed that VideoA11y descriptions matched or exceeded the quality of trained human annotations in most metrics. Notably, VideoA11y outperformed trained human annotators on the clarity metric, with a statistically significant improvement (<em>p = 0.004</em>). Participants highlighted that VideoA11y descriptions were often clearer, better synchronized with the visual content, and provided unbiased and detailed accounts of the videos. These findings demonstrate that VideoA11y can generate video descriptions that rival professional standards, making it a valuable tool for scaling video accessibility without compromising quality.
          </p>
          <h4>Study 4</h4>
          <p>
            Study 4 evaluated the quality of VideoA11y descriptions compared to those created by trained human annotators. This study aimed to assess whether descriptions generated by VideoA11y could meet or exceed the standards of professional audio describers who followed curated AD guidelines.
          </p>
          <p>
            A total of 47 videos were selected from YouTube, spanning a variety of categories such as sports, instructional content, and entertainment. These videos had an average duration of 4.92 minutes. A team of four accessibility researchers carefully crafted descriptions for these videos, adhering strictly to the 42 professional AD guidelines compiled for this project. These human-generated descriptions were then compared to descriptions created by VideoA11y using GPT-4 Vision.
          </p>
          <img src="assets/images/study-4.png" alt="Results of Study 4 with seven professional audio describers. VideoA11y performed similarly to trained humans on all metrics (p>0.05).">
          <p>
            To evaluate the descriptions, 47 sighted participants from Amazon Mechanical Turk (MTurk) were recruited. Each participant reviewed descriptions generated by both methods for the same video and rated them on four key metrics: descriptiveness, objectivity, accuracy, and clarity. The participants used a 5-point scale to provide their ratings.
          </p>
          <p>
            Results showed that VideoA11y descriptions matched or exceeded the quality of trained human annotations in most metrics. Notably, VideoA11y outperformed trained human annotators on the clarity metric, with a statistically significant improvement (<em>p = 0.004</em>). Participants highlighted that VideoA11y descriptions were often clearer, better synchronized with the visual content, and provided unbiased and detailed accounts of the videos. These findings demonstrate that VideoA11y can generate video descriptions that rival professional standards, making it a valuable tool for scaling video accessibility without compromising quality.
          </p>
          <h4>Study 5</h4>
          <p>
            Study 5 focused on evaluating the effectiveness of VideoA11y and its impact on the video-watching 
            experience of blind and low vision (BLV) individuals. This study aimed to compare descriptions 
            generated by VideoA11y to novice human annotations and assess user preferences and satisfaction.
          </p>
          <p>
            A total of 40 BLV participants were recruited, including six completely blind and 34 legally blind 
            individuals with varying degrees of visual impairment. Participants were divided into two groups of 
            20, with each group evaluating five videos across five categories: entertainment, how-to, sports, 
            pets and animals, and people and vlogs. For each video, participants experienced both human-generated 
            descriptions and VideoA11y-generated descriptions, presented in a counterbalanced order.
          </p>
          <img src="assets/images/study-4-1.png" alt="VideoA11y outperforms novice human annotations in all five video
          categories and outperforms novice human annotations in all four metrics.">
          <p>
            Participants rated the descriptions on a 10-point Likert scale across four metrics: descriptiveness, 
            objectivity, accuracy, and clarity. They also indicated their preferred description for each video 
            and provided feedback on why they made their choice. Descriptions were embedded into the videos as 
            audio, following best practices for accessibility.
          </p>
          <p>
            The results were highly positive for VideoA11y. Participants preferred VideoA11y-generated descriptions 
            in over 90% of cases, with an overall selection rate of 180 out of 200 comparisons. VideoA11y achieved 
            significantly higher ratings than novice human annotations on all metrics. Comments from BLV participants 
            highlighted the clarity, detail, and alignment of VideoA11y descriptions with the visual content, 
            enhancing their understanding and enjoyment of the videos.
          </p>
          <img src="assets/images/study-4-2.png" alt="Overall pairwise comparisons from BLV user evaluations between VideoA11y and novice human descriptions in Study 5.">
          <p>
            This study underscores the ability of VideoA11y to meet the unique needs of BLV users, providing accurate, 
            clear, and descriptive audio that enhances their accessibility to video content.
          </p>
    </div>
    <div class="article-text mt-5">
        <h2>Technical Benchmarks</h2>
        <h4>Overview of Benchmarking</h4>
        <p>
            VideoA11y was evaluated using a benchmark designed to test the performance of state-of-the-art (SOTA) open-source models in generating accessible video descriptions. These experiments compared baseline models to fine-tuned versions trained on the VideoA11y-40K dataset. The goal was to determine whether training on VideoA11y-40K improved model performance on both standard and custom metrics.
        </p>
        <h4>Baseline Models</h4>
        <p>Four SOTA open-source models were selected for comparison:</p>
        <ul>
            <li><strong>Video-LLaVA-7B:</strong> A smaller, lightweight model.</li>
            <li><strong>VILA1.5-40B:</strong> Focused on cross-modality analysis.</li>
            <li><strong>LLaVA-NeXT-Video-32B:</strong> Specialized in understanding temporal changes.</li>
            <li><strong>LLaVA-OneVision-72B:</strong> A large-scale model for video comprehension.</li>
        </ul>
        <p>
            Each model was evaluated under its original settings, including the number of frames processed and inference parameters. These served as baselines to assess the impact of fine-tuning on VideoA11y-40K.
        </p>
        <h4>Fine-Tuned Models</h4>
        <p>
            Two open-source models, <strong>Video-LLaVA-7B</strong> and <strong>LLaVA-NeXT-Video-32B</strong>, were fine-tuned on VideoA11y-40K. This involved LoRA fine-tuning, which adjusts specific model layers while preserving pre-trained knowledge. The fine-tuning process included:
        </p>
        <ul>
            <li>10 epochs</li>
            <li>Learning rate of 2e-5</li>
            <li>Batch size of 4 per device</li>
            <li>Support for up to 32,768 token lengths</li>
        </ul>
        <p>The fine-tuned models were named <strong>VideoA11y-7B</strong> and <strong>VideoA11y-32B</strong>.</p>
        <h4>Evaluation Metrics</h4>
        <p>Evaluations were conducted using both standard NLP metrics and custom accessibility-focused metrics:</p>
        <ul>
            <li>
            <strong>Standard Metrics:</strong> BLEU, METEOR, ROUGE, CIDEr, and SPICE, which assess text coherence, relevance, and alignment with human judgment.
            </li>
            <li>
            <strong>Custom Metrics:</strong> Descriptiveness, objectivity, accuracy, and clarity, rated on a scale of 1–5. These metrics reflect the specific needs of BLV users.
            </li>
        </ul>
        <h4>Benchmarking Results</h4>
        <img src="assets/images/scores-1.png" alt="Comparison of standard NLP metrics for di!erent models on a held-out test set. Bold number indicate the best performance, and underlined number indicate the second best performance.">
        <p>
            Fine-tuned models consistently outperformed baseline models across all metrics. Key findings include:
        </p>
        <ul>
            <li>
            <strong>Standard Metrics:</strong> VideoA11y-32B achieved the highest scores in BLEU, METEOR, CIDEr, and SPICE, followed by VideoA11y-7B. These models demonstrated improved semantic richness and alignment with ground truth.
            </li>
            <li>
            <strong>Custom Metrics:</strong> Evaluations using GPT-4 as an evaluator showed that VideoA11y-32B achieved the highest ratings on descriptiveness, objectivity, accuracy, and clarity. VideoA11y-7B also performed well, surpassing baseline models.
            </li>
        </ul>
        <img src="assets/images/scores-2.png" alt="Comparison of custom metrics for di!erent models on a held-out test set. Bold number indicate the best performance, and underlined number indicate the second best performance.">
        <h4>Implications</h4>
        <p>
            Fine-tuning on VideoA11y-40K significantly enhances the capability of open-source models to generate high-quality, accessible video descriptions. While the fine-tuned models are not yet at the level of proprietary systems like GPT-4 Vision, they offer a scalable and cost-effective solution for video accessibility. This makes them ideal for platforms with large volumes of video content.
        </p>
    </div>
    <div class="article-text mt-5">
        <h2>Conclusion and Future Discussions</h2>
        <p>
            VideoA11y addresses a critical gap in video accessibility by leveraging advanced AI and professionally curated guidelines. Its scalable method and comprehensive dataset enable the creation of high-quality video descriptions tailored to BLV users. User evaluations confirmed its superior performance over human-generated descriptions, making it a transformative tool for inclusive content.
        </p>
        <p>
            Future work will explore personalization, such as adapting descriptions to individual preferences and integrating inline descriptions that seamlessly fit video pacing. These advancements will ensure that VideoA11y continues to lead in video accessibility, creating a richer experience for BLV users and setting a new standard in inclusive technology.
        </p>
    </div>




    <div class="article-text mt-5">
        <h2>Sample Videos</h2>
        <p>Below are a few videos from the VideoA11y dataset with revised descriptions using VideoA11y.</p>
    </div>
    <div class="video-container">
        <button id="prev" class="btn">&#10094;</button>
        <video id="videoPlayer1" controls>
            <source src="assets/videos/vid1-example.mp4" type="video/mp4">
            Your browser does not support the video tag.
        </video>
        <div id="caption1" class="caption-box">
            <h5 class="cap-title">Video #1</h5>
            <p><strong>Video ID:</strong> FveaOFL7eOs</p>
            <p><strong>VideoA11y:</strong> A person in a white protective beekeeping suit
                inspects a large rectangular honeycomb frame. The frame is mostly covered with a pattern
                of hexagonal cells, some filled with honey, while others are empty. The beekeeper holds
                the frame with both hands, examining it closely. Bees crawl across the surface, their
                buzzing audible. Text on the screen questions the absence of eggs or larvae in the brood
                chamber.</p>
        </div>
        <video id="videoPlayer2" controls style="display:none;">
            <source src="assets/videos/vid2-example.mp4" type="video/mp4">
            Your browser does not support the video tag.
        </video>
        <div id="caption2" class="caption-box" style="display:none;">
            <h5 class="cap-title">Video #2</h5>
            <p><strong>Video ID:</strong> jjOqzXCwDec</p>
            <p><strong>VideoA11y:</strong> A tabby cat peeks through a small, square pet door
                built into a glass patio door, then steps out onto a brick patio. The scene shifts to
                the cat confidently striding towards the camera. Subsequent frames show the brand name
                'SUREFLAP' in bold white letters on a blue background, followed by the slogan 'SureFlap
                pets are happy pets' in white script.</p>
        </div>
        <video id="videoPlayer3" controls style="display:none;"> 
            <source src="assets/videos/vid3-example.mp4" type="video/mp4">
            Your browser does not support the video tag.
        </video>
        <div id="caption3" class="caption-box" style="display:none;">
            <h5 class="cap-title">Video #3</h5>
            <p><strong>Video ID:</strong> ucaCmhNo78k</p>
            <p><strong>VideoA11y:</strong> The video demonstrates the preparation of a cheesy
                hash brown casserole. The cook starts by greasing a clear glass baking dish. Cream of
                chicken soup is poured into the dish, followed by the addition of melted margarine. The
                cook then sprinkles shredded Colby cheese into the mixture. After each ingredient is
                added, the cook mixes them thoroughly. Diced onions are scattered over the mixture,
                seasoned with salt and black pepper, and stirred in. Finally, shredded hash browns are
                combined with the rest of the ingredients. The cook ensures an even distribution of all
                components before transferring the dish to the oven to bake until golden brown. The
                finished casserole is served on a plate, showcasing a crispy golden top with a creamy,
                cheesy interior.</p>
        </div>
        <button id="next" class="btn">&#10095;</button>
    </div>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
        let currentVideoIndex = 1;
        const totalVideos = 4;

        function showVideo(index) {
            for (let i = 1; i <= totalVideos; i++) {
            const video = document.getElementById(`videoPlayer${i}`);
            const caption = document.getElementById(`caption${i}`);
            if (video && caption) {
                video.style.display = "none";
                caption.style.display = "none";
            }
            }

            document.getElementById(`videoPlayer${index}`).style.display = "block";
            document.getElementById(`caption${index}`).style.display = "block";

            updateButtonVisibility(index);
        }

        function updateButtonVisibility(index) {
            const prevButton = document.getElementById("prev");
            const nextButton = document.getElementById("next");

            if (index === 1) {
            prevButton.style.display = "none";
            } else {
            prevButton.style.display = "inline-block";
            }
            if (index === totalVideos) {
            nextButton.style.display = "none";
            } else {
            nextButton.style.display = "inline-block";
            }
        }

        document.getElementById("next").addEventListener("click", function () {
            if (currentVideoIndex < totalVideos) {
            currentVideoIndex++;
            showVideo(currentVideoIndex);
            }
        });

        document.getElementById("prev").addEventListener("click", function () {
            if (currentVideoIndex > 1) {
            currentVideoIndex--;
            showVideo(currentVideoIndex);
            }
        });

        showVideo(currentVideoIndex);
        });
    </script>
</body>

</html>
